{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7579357d-091b-4e01-88d6-93e5debc915f",
   "metadata": {},
   "source": [
    "##Report on Alphabet Soup Charity Fund Prdictor\n",
    "\n",
    "Alphabet Soup funds over 34,00 organizations. It is using an algorithm, created with neural networks for machine learning, to predict whether or not applicants for funding will be successful.\n",
    "\n",
    "Within this dataset are a number of columns that capture metadata about each organization, such as the following:\n",
    "\n",
    "* **EIN** and **NAME**—Identification columns\n",
    "* **APPLICATION_TYPE**—Alphabet Soup application type\n",
    "* **AFFILIATION**—Affiliated sector of industry\n",
    "* **CLASSIFICATION**—Government organization classification\n",
    "* **USE_CASE**—Use case for funding\n",
    "* **ORGANIZATION**—Organization type\n",
    "* **STATUS**—Active status\n",
    "* **INCOME_AMT**—Income classification\n",
    "* **SPECIAL_CONSIDERATIONS**—Special consideration for application\n",
    "* **ASK_AMT**—Funding amount requested\n",
    "* **IS_SUCCESSFUL**—Was the money used effectively\n",
    "\n",
    "An analyisis and optimization of the algorith were perform under the following parameters:\n",
    "\n",
    "* The target variable for the model was identifies as \"IS_SUCCESFUL\" (1=yes, 0=no).\n",
    "* Data procesing to get rid of irrelevant data and columns was performed.\n",
    "* The columns droped were: EIN, NAME CLASSIFICATION and APPLICATION TYPE.\n",
    "* Outliers were identified through unique values and calssified as other in various code lines.\n",
    "* Normalization was performed by using pd.get_dummies().\n",
    "\n",
    "# Modeling\n",
    "\n",
    "Two neural network models were created. \n",
    "\n",
    "The first model is Charity Fund predictor used the following parameters:\n",
    "\n",
    "   * Neurons: since we had 50 columns the input nodes were calculated to be 50\n",
    "    * Four hidden layers were created (1:100 nodes, 2:50 nodes, 3:20 nodes, 4:4 nodes)\n",
    "    * Relu was used as the activation function for the hidden layers and sigmoid for the output layer\n",
    "    * The overall performance was Loss: 0.5941712260246277, Accuracy: 0.7245481014251709, the target .75 accuracy was not achieved\n",
    "    * To increase performance a second neural network model was created.\n",
    "    \n",
    "The second model, Alphabet-Soup-Charity-Optimizer used the following parameters:\n",
    "\n",
    "   * Neurons: since we had 38 columns the input nodes were calculated to be 38\n",
    "    * Two hidden layers were created (1:114 nodes, 2:28)\n",
    "    * Tanh was used as the activation function for the hidden layers and sigmoid for the output layer\n",
    "    * The overall performance was 0.573111355304718, Accuracy: 0.7243148684501648 the target .75 accuracy was not achieved\n",
    "    \n",
    "    ![performance.png](performance.png)\n",
    "    \n",
    "\n",
    "#Summary\n",
    "\n",
    "Overall results through various trials conclude that neural network may not be the best model for the data and it is recomended to use clustering for next trias. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
